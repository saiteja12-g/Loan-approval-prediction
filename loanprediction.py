# -*- coding: utf-8 -*-
"""Final Assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i7PoXugkXSoArqZyLVfxSG9P8AC_90YO

# **Loan Approval Prediction**
In this problem, we will try to decide where a particular ID get the loan. we take the decision by different machine learning algoritms
"""

import pandas as pd
import matplotlib.pyplot as plot
import seaborn as sb

train_df = pd.read_csv("18210.csv")    #importing the dataset
print(train_df.info())                 #Brief description about the dataset

"""We can clearly see there are 13 columns and there are missing data points in the dataset.

"""

train_df = train_df.drop(columns= ["Loan_ID"])

"""

# Analyzing the dataset by graphs

By looking at the graphs, We will try to get some conclusions. This is a very important step. All having a data like this, We can 

1) generalise or remove the columns which doesnt have any 
significane on making the decision. 

2) remove the outliners. On large datasets, the computation can  be decreased."""

sb.countplot(x=train_df['Gender'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

Most of the people who apply are men (3 times more) and there are higher chances the loan gets sanctioned if the applicant sex is 'Male'
"""

sb.countplot(x=train_df['Married'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

2/3 rd of the people who applied for loan are married and married applicants are more likely to get loan sanctioned
"""

sb.countplot(x=train_df['Dependents'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

If the dependants are 0, there is highly likely that loan is approved
"""

sb.countplot(x=train_df['Education'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

Graduated population are more likely to get the loan
"""

sb.countplot(x=train_df['Self_Employed'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

5/6th of population is not self employed
"""

sb.countplot(x=train_df['Credit_History'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

Applicant with credit history are far more likely to be accepted.
"""

sb.countplot(x=train_df['Loan_Status'], data=train_df, hue='Loan_Status')

"""**Interpreted data from above graph**

We can conclude, 2/3 of people who applied got there loan sanctioned.

# **Preprocessing the data**
The data needs to be pre processed to fit to the model. 2 reasons why this is mandatory is 


*   Missing values (Imputing)
*   few columns have features in test format. these needs to be converted in number format (Encoding).
"""

train_df_encoded = pd.get_dummies(train_df,drop_first=True) #Convert categorical variable into dummy/indicator variables.
print(train_df_encoded.head())

X = train_df_encoded.drop(columns='Loan_Status_Y')
Y = train_df_encoded['Loan_Status_Y']

g = sb.lmplot(x='ApplicantIncome',y='LoanAmount',data= train_df_encoded , col='Self_Employed_Yes', hue='Gender_Male',
          palette= ["Red" , "Blue","Yellow"] ,aspect=1.2,size=6)
g.set(ylim=(0, 800))

"""Above graph tells:

* The male applicants take more amount of loan than female.
* The males are higher in number of "NOT self employed" category.
* Majority of applicants are NOT self employed.
* The majority of income taken is about 0-200 with income in the range 0-20000.

# Train and Test split
"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20,stratify =Y)

from sklearn.impute import SimpleImputer
temp = SimpleImputer(strategy='mean')
X_train = temp.fit_transform(X_train)
X_test = temp.fit_transform(X_test)

"""# LINEAR REGRESSION FOR CLASSIFICATION"""

from sklearn.linear_model import LinearRegression
lrc = LinearRegression()
lrc.fit(X_train, Y_train)
temp= lrc.predict(X_test)
Y_pred_lrc = []

for i in temp:
  if i>0.5:
    Y_pred_lrc.append(1)
  else:
    Y_pred_lrc.append(0)

from sklearn.metrics import accuracy_score
# Accuracy score
result2 = accuracy_score(Y_test, Y_pred_lrc)
print("\nAccuracy:",result2)

"""# LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression(solver='lbfgs')
LR.fit(X_train, Y_train)
Y_pred_LR = LR.predict(X_test)

from sklearn.metrics import accuracy_score

result2 = accuracy_score(Y_test, Y_pred_LR)
print("\nAccuracy:",result2)

"""# SUPPORT VECTOR MACHINES (SVM)






"""

from sklearn.svm import SVC

SVM = SVC(kernel='linear')
SVM.fit(X_train, Y_train) 
Y_pred_SVM = SVM.predict(X_test)

# Accuracy score
result2 = accuracy_score(Y_test, Y_pred_SVM)
print("\nAccuracy:",result2)

"""# RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier(n_estimators= 100)
random_forest.fit(X_train, Y_train)
Y_pred_rf = random_forest.predict(X_test)

# Accuracy score
result2 = accuracy_score(Y_test, Y_pred_rf)
print("\nAccuracy:",result2)

"""# NEURAL NETWORK"""

from keras.layers import Dropout
from keras import regularizers
from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(16 ,activation='relu',input_shape=(14,))) 
model.add(Dense(16,activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])

hist = model.fit(X_train, Y_train,epochs=300,verbose=1, validation_split=0.1)
accuracy = model.evaluate(X_test,Y_test)[1]

plot.plot(hist.history['loss'])
plot.plot(hist.history['val_loss'])
plot.title('Model loss')
plot.ylabel('Loss')
plot.xlabel('Epoch')
plot.legend(['Train', 'Val'], loc='upper right')
plot.show()
plot.plot(hist.history['accuracy'])
plot.plot(hist.history['val_accuracy'])
plot.title('Model acc')
plot.ylabel('acc')
plot.xlabel('Epoch')
plot.legend(['Train', 'Val'], loc='upper right')
plot.show()

"""# UNSUPERVISED LEARNING - K means clusterning"""

from sklearn.cluster import KMeans

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_train)
    wcss.append(kmeans.inertia_)
plot.plot(range(1, 11), wcss)
plot.title('Elbow Method')
plot.xlabel('Number of clusters')
plot.ylabel('WCSS')
plot.show()
kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=10, random_state=0)
pred_y_kn = kmeans.fit_predict(X_train)
result2 = accuracy_score(Y_train, pred_y_kn)
print("\nAccuracy:",result2)

"""**ON UNSEEN DATA BY LOGISTIC REGRESSION**"""

data1 = pd.read_csv('test.csv')
data1 = data1.drop(columns=['Loan_ID'])
data1_df_encoded = pd.get_dummies(data1,drop_first=True)
data1_df_encoded.head()
imp1 = SimpleImputer(strategy='mean')
imp1_train = imp1.fit(data1_df_encoded)
data1_df_encoded = imp1_train.transform(data1_df_encoded)
predict= LR.predict(data1_df_encoded)
data1['Y/N']= predict
data1.to_csv('predicted.csv')

